[
  {
    "objectID": "dw_eda.html",
    "href": "dw_eda.html",
    "title": "Spotify rewrapped",
    "section": "",
    "text": "Static analysis\nAfter we have imported the libraries we can download the extended data that we gathered from our spotify download. Since all the files I need are .json I use the pattern *json to load them all in at once instead of individually loading them in.\n\npath &lt;- \"./my_extended_data/\"\nfiles &lt;- dir(path, pattern = \"*.json\")\n\ndata &lt;- read_csv(\"./my_extended_data/my_data.csv\")\n\nRows: 139773 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): master_metadata_track_name, master_metadata_album_artist_name, mas...\ndbl  (1): ms_played\ndttm (1): ts\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nTo get a better understanding of what all the data fields are make sure to look through the Start Here page\n\nWe can also use glimpse to take a quick look at the data to see what type each column is\n\nglimpse(data)\n\nRows: 139,773\nColumns: 8\n$ ts                                &lt;dttm&gt; 2020-06-25 13:29:16, 2023-02-06 17:…\n$ ms_played                         &lt;dbl&gt; 1130, 162577, 214493, 60089, 150418,…\n$ master_metadata_track_name        &lt;chr&gt; \"Pray ft. Kameron (Blanke Remix)\", \"…\n$ master_metadata_album_artist_name &lt;chr&gt; \"ILLENIUM\", \"Waterparks\", \"The Amity…\n$ master_metadata_album_album_name  &lt;chr&gt; \"Pray (ft. Kameron) - Remixes\", \"FAN…\n$ reason_end                        &lt;chr&gt; \"fwdbtn\", \"trackdone\", \"trackdone\", …\n$ reason_start                      &lt;chr&gt; \"fwdbtn\", \"trackdone\", \"clickrow\", \"…\n$ spotify_track_uri                 &lt;chr&gt; \"spotify:track:6BzTAVtYRHbEx6gQ1yMwZ…\n\n\nAfter looking through the columns we want to save the ones that we will be using for the analysis.\n\nspotify_data &lt;- data %&gt;%\n  select(ts, ms_played, master_metadata_track_name, master_metadata_album_artist_name, master_metadata_album_album_name, reason_start, reason_end)\n\n\n\nData Wrangling Date Time & Rename Column\nThroughout the semester I learned how important tidy data is. Here we will implement two of the changes to make this dataframe easier to manage.\n\nConverting a column from char to POSIXct. This will allow for easier data analysis since the column will be of type date. We can use the lubridate to gain more information.\nRenaming columns for easier analysis\n\n\nspotify_data &lt;- spotify_data %&gt;%\n  as_tibble() %&gt;%\n  separate(\n    col = \"ts\",\n    into = c(\"date\", \"time\"),\n    sep = \"T\"\n  ) %&gt;%\n  separate(\n    col = \"time\",\n    into = \"time\",\n    sep = \"Z\"\n  )\n\nUpdate to POSIX\n\ndatetime &lt;- as.POSIXct(paste(spotify_data$date, spotify_data$time), format = \"%Y-%m-%d %H:%M:%S\")\nspotify_data$datetime &lt;- datetime\n\nHere we can check that the change was implimented\n\nspotify_data %&gt;%\n  select(datetime)\n\n# A tibble: 139,773 × 1\n   datetime           \n   &lt;dttm&gt;             \n 1 2020-06-25 13:29:16\n 2 2023-02-06 17:29:01\n 3 2018-11-21 04:38:02\n 4 2021-06-26 20:02:50\n 5 2022-07-11 19:57:59\n 6 2021-03-08 18:55:01\n 7 2020-04-09 18:23:56\n 8 2020-11-29 19:29:37\n 9 2022-11-13 23:39:40\n10 2021-03-30 01:00:14\n# … with 139,763 more rows\n\n\nFinally, we just need to rename the columns.\n\nspotify_data &lt;- spotify_data %&gt;%\n  rename(\n    \"track\" = \"master_metadata_track_name\",\n    \"artist\" = \"master_metadata_album_artist_name\",\n    \"album\" = \"master_metadata_album_album_name\"\n  )\n\n\nNow that we have cleaned up the data a bit we can move into data exploration and analysis!\n\n\n\nTotal Songs Played\nI’ve used this spotify account since 2018. I know I really enjoy listening to music all the time. The first insight I want to gain is on which day of the week do I listen to the most music.\n\nlibrary(lubridate)\nviz &lt;- spotify_data %&gt;%\n  drop_na(datetime) %&gt;%\n  mutate(weekday = wday(datetime, label = TRUE)) %&gt;%\n  group_by(weekday) %&gt;%\n  count(weekday, na.rm = T) %&gt;%\n  ggplot(aes(weekday, n)) +\n  geom_point() +\n  labs(\n    x = \"day of the week\",\n    y = \"number of songs played from 2018-2023\",\n  )\n\nIt looks like my Monday’s typically consist of a cup of coffee and some music to start my week off right!\n\n\ntotal songs played during week plot\n\n\n\n\n\n\n\n\n\n\n\nTop songs no skip\nThe analysis for my top 5 tracks tracked every time the song was played. However, just because a song plays doesn’t mean I listened to it all the way through. This plot is for songs I listened to fully.\n\nskipped &lt;- spotify_data %&gt;%\n  # select(9:11,17,23) %&gt;%\n  # na.omit() %&gt;%\n  filter(reason_end == \"trackdone\") %&gt;%\n  mutate(year = year(datetime)) %&gt;%\n  group_by(year, track) %&gt;%\n  summarise(n = n()) %&gt;%\n  top_n(n = 5, wt = n)\n\n\nskipped\n\n# A tibble: 41 × 3\n# Groups:   year [7]\n    year track                                         n\n   &lt;dbl&gt; &lt;chr&gt;                                     &lt;int&gt;\n 1  2018 Care                                         38\n 2  2018 Clever                                       39\n 3  2018 FEEL NOTHING                                 49\n 4  2018 Overthinking                                 70\n 5  2018 Set Me Free                                  46\n 6  2019 Bow Down                                    116\n 7  2019 Crop Circles                                 79\n 8  2019 Good Things Fall Apart (with Jon Bellion)    73\n 9  2019 Head Hunter                                  85\n10  2019 We Own The Night                             97\n# … with 31 more rows\n\n\n\ndone &lt;- ggplot(skipped, aes(fill = track, x = year, y = n)) +\n  geom_bar(position = \"stack\", stat = \"identity\", show.legend = FALSE) +\n  scale_x_continuous(breaks = c(2018:2023))\n\n\n\nVisualization\n\ndone_plotly &lt;- ggplotly(done + labs(title = \"Top 5 Songs Throughout The Years - Track Done\")) %&gt;%\n  layout(showlegend = FALSE)\n\nWarning: Removed 11 rows containing missing values (position_stack).\n\ndone_plotly\n\n\n\n\n\n\n\nHow often did I listen to my top song\nOnce I filtered by the songs that finished playing without me skipping you can see that my top 5 tracks change throughout the year\nMy top song in 2019 was Bow Down from I Prevail played a total of 116 times. Which day of the week did I listen to the song the most?\n\namount_played_2019 &lt;- spotify_data %&gt;%\n  drop_na(datetime) %&gt;%\n  mutate(\n    weekday = wday(datetime, label = TRUE),\n    year = year(datetime)\n  ) %&gt;%\n  select(weekday, year, track, datetime) %&gt;%\n  na.omit() %&gt;%\n  group_by(track, weekday, year) %&gt;%\n  count() %&gt;%\n  filter(track == \"Bow Down\", year == 2019)\n\n\namount_played_2019\n\n# A tibble: 7 × 4\n# Groups:   track, weekday, year [7]\n  track    weekday  year     n\n  &lt;chr&gt;    &lt;ord&gt;   &lt;dbl&gt; &lt;int&gt;\n1 Bow Down Sun      2019    21\n2 Bow Down Mon      2019    36\n3 Bow Down Tue      2019    29\n4 Bow Down Wed      2019    26\n5 Bow Down Thu      2019    23\n6 Bow Down Fri      2019    24\n7 Bow Down Sat      2019    13\n\n\nThis table shows that in 2019 on in total on Mondays I listened to Bow Down the most. Which makes sense becasue I typically went to the gym on monday mornings that yearr.\n\n\nHow many tracks do I skip a year?\nAfter seeing how my top 5 changed when i filtered out by songs that finished. I was curious to see how many songs I skip every year.\n\nskipped_yearly &lt;- spotify_data %&gt;%\n  mutate(year = year(datetime)) %&gt;%\n  select(track, reason_end, year) %&gt;%\n  na.omit() %&gt;%\n  group_by(year, reason_end) %&gt;%\n  count()\n\n\nskipped_yearly\n\n# A tibble: 60 × 3\n# Groups:   year, reason_end [60]\n    year reason_end                       n\n   &lt;dbl&gt; &lt;chr&gt;                        &lt;int&gt;\n 1  2018 backbtn                        117\n 2  2018 endplay                       1141\n 3  2018 fwdbtn                        2678\n 4  2018 logout                          63\n 5  2018 remote                          28\n 6  2018 trackdone                     2969\n 7  2018 trackerror                       6\n 8  2018 unexpected-exit                  5\n 9  2018 unexpected-exit-while-paused   209\n10  2018 unknown                          1\n# … with 50 more rows\n\n\n\nskipped_yearly_plot &lt;- ggplot(data = skipped_yearly, aes(x = year, y = n, fill = reason_end)) +\n  geom_point()\n\n\n\nVisualization\nThis is the rendered graphic showing how the songs I listened to ended throughout the years.\n\nggplotly(skipped_yearly_plot)\n\n\n\n\n\n\n\nTotal listening hours\nI know I spend a lot of time listening to music so I wanted to visualize this.\n\nyear_plot &lt;- spotify_data %&gt;%\n  mutate(date = date(datetime)) %&gt;%\n  group_by(date = floor_date(date, \"week\")) %&gt;%\n  summarise(total_ms_played = sum(ms_played)) %&gt;%\n  na.omit() %&gt;%\n  mutate(\n    second = (total_ms_played / 1000),\n    minute = (second / 60),\n    hours = (minute / 60)\n  ) %&gt;%\n  ggplot(aes(x = date, y = hours)) +\n  geom_col(aes(fill = hours)) +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"12 month\") +\n  scale_fill_gradient(low = \"grey\", high = \"gold\") +\n  labs(title = \"Playback hours per week\") +\n  xlab(\"\") +\n  theme_light()\n\n\n\nVisualization\nThe bar graph below shows how many hours a week I spent listening to music.\n\nggplotly(year_plot + labs(title = \"Hours spent listening to music weekly\"))\n\n\n\n\n\nI was curious to see what I was doing at that time… I had a couple of road trips so it makes sense that my listening hours were at an all time high!\n\n\nBreakdown of the amount of songs I listened to that week\nBecause the highest week I had of listening to music I was curious to see how many songs I listened to each day\n\nweek_of_oct &lt;- spotify_data %&gt;%\n  mutate(date = date(datetime)) %&gt;%\n  select(ms_played, track, date) %&gt;%\n  filter(date %in% ymd(c(\"2020-10-25\", \"2020-10-26\", \"2020-10-27\", \"2020-10-28\", \"2020-10-29\", \"2020-10-30\", \"2020-10-31\"))) %&gt;%\n  group_by(track, date) %&gt;%\n  mutate(amount_played = n()) %&gt;%\n  group_by(date) %&gt;%\n  summarise(\n    total_play = sum(amount_played)\n  )\n\n\nweek_of_oct_plot &lt;- ggplot(data = week_of_oct, aes(x = date, y = total_play)) +\n  geom_bar(position = \"stack\", stat = \"identity\", show.legend = FALSE) +\n  scale_x_date(date_labels = \"%b %D\", date_breaks = \"1 day\") +\n  theme(axis.text = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\nggplotly(week_of_oct_plot) %&gt;%\n  layout(showlegend = FALSE)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spotify rewrapped",
    "section": "",
    "text": "This project uses R and the Spotify API to scrape the data. From there, it will be analyzed and preprocessed in the R language in an exploratory data analysis.\nWe can visualize all this data using the web application framework, Shiny, in Python. All the data and code to run the project will be available on GitHub. The choice to use both R and Python in one project was to highlight the availability of tools to make multi-language products. This also allows us to use tools that are best suited for each task, eg, data exploration in R and machine learning in Python."
  },
  {
    "objectID": "static-analysis.html",
    "href": "static-analysis.html",
    "title": "Static Analysis",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten"
  },
  {
    "objectID": "static-analysis.html#show-my-most-saved-artist",
    "href": "static-analysis.html#show-my-most-saved-artist",
    "title": "Static Analysis",
    "section": "Show my most saved artist",
    "text": "Show my most saved artist\n\nmyDataJson &lt;- fromJSON(\"./MyData/YourLibrary.json\")\n\n\nmyDataJson$tracks %&gt;%\n  select(artist, album, track) %&gt;%\n  count(artist, sort = T) %&gt;%\n  head(5)\n\n                artist  n\n1 Bring Me The Horizon 46\n2        Of Mice & Men 31\n3    Dance Gavin Dance 29\n4            I Prevail 23\n5            Beartooth 19\n\n\n##Streaming History\n\ns0 &lt;- myDataJson &lt;- fromJSON(\"./MyData/StreamingHistory0.json\")\n\n\ns1 &lt;- myDataJson &lt;- fromJSON(\"./MyData/StreamingHistory1.json\")\n\n\ns2 &lt;- myDataJson &lt;- fromJSON(\"./MyData/StreamingHistory2.json\")\n\n\nglimpse(s0)\n\nRows: 10,000\nColumns: 4\n$ endTime    &lt;chr&gt; \"2022-03-20 16:41\", \"2022-03-21 03:11\", \"2022-03-21 14:45\",…\n$ artistName &lt;chr&gt; \"AnnenMayKantereit\", \"NGHTMRE\", \"AnnenMayKantereit\", \"ILLEN…\n$ trackName  &lt;chr&gt; \"Tom's Diner\", \"Shady Intentions (feat. Tori Levett)\", \"Tom…\n$ msPlayed   &lt;int&gt; 200384, 169250, 271296, 1962, 85, 57073, 1151, 166122, 2210…\n\n\n\nglimpse(s1)\n\nRows: 10,000\nColumns: 4\n$ endTime    &lt;chr&gt; \"2022-08-24 14:31\", \"2022-08-24 14:32\", \"2022-08-24 14:34\",…\n$ artistName &lt;chr&gt; \"Zeds Dead\", \"Beartooth\", \"Jack Harlow\", \"Louis The Child\",…\n$ trackName  &lt;chr&gt; \"We Could Be Kings\", \"Pick Your Poison\", \"First Class\", \"We…\n$ msPlayed   &lt;int&gt; 112431, 43630, 17345, 134559, 20015, 22035, 75441, 243226, …\n\n\n\nglimpse(s2)\n\nRows: 3,073\nColumns: 4\n$ endTime    &lt;chr&gt; \"2023-02-16 00:01\", \"2023-02-16 00:40\", \"2023-02-16 00:40\",…\n$ artistName &lt;chr&gt; \"Måneskin\", \"Beartooth\", \"Bring Me The Horizon\", \"AWOLNATIO…\n$ trackName  &lt;chr&gt; \"Beggin'\", \"I Have a Problem\", \"Throne\", \"Jailbreak\", \"Wick…\n$ msPlayed   &lt;int&gt; 119803, 2048, 119936, 1088, 271800, 223218, 18285, 162577, …\n\n\nsince they all have the same columns I want to combine them to create one large dataframe\n\nstreaming &lt;- rbind(s0, s1, s2)\n\n\nglimpse(streaming)\n\nRows: 23,073\nColumns: 4\n$ endTime    &lt;chr&gt; \"2022-03-20 16:41\", \"2022-03-21 03:11\", \"2022-03-21 14:45\",…\n$ artistName &lt;chr&gt; \"AnnenMayKantereit\", \"NGHTMRE\", \"AnnenMayKantereit\", \"ILLEN…\n$ trackName  &lt;chr&gt; \"Tom's Diner\", \"Shady Intentions (feat. Tori Levett)\", \"Tom…\n$ msPlayed   &lt;int&gt; 200384, 169250, 271296, 1962, 85, 57073, 1151, 166122, 2210…\n\n\n\nstreaming %&gt;%\n  head(5)\n\n           endTime        artistName                            trackName\n1 2022-03-20 16:41 AnnenMayKantereit                          Tom's Diner\n2 2022-03-21 03:11           NGHTMRE Shady Intentions (feat. Tori Levett)\n3 2022-03-21 14:45 AnnenMayKantereit                          Tom's Diner\n4 2022-03-21 14:46          ILLENIUM            Hold On (with Georgia Ku)\n5 2022-03-21 14:46          ILLENIUM            Hold On (with Georgia Ku)\n  msPlayed\n1   200384\n2   169250\n3   271296\n4     1962\n5       85\n\n\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n\nmin(streaming$endTime)\n\n[1] \"2022-03-20 16:41\"\n\n\n\nmax(streaming$endTime)\n\n[1] \"2023-03-21 23:44\"\n\n\n\nstreaming %&gt;%\n  mutate(weekday = wday(endTime)) %&gt;%\n  group_by(weekday) %&gt;%\n  count(weekday) %&gt;%\n  ggplot(aes(weekday, n)) +\n  geom_point() +\n  labs(\n    x = \"date\",\n    y = \"number of songs played\"\n  )"
  }
]